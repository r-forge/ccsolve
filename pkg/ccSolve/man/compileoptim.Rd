\name{compile.optim}
\alias{compile.optim}
\alias{ccoptim}

\title{Compiled code optimization  }
\description{
  \code{compile.optim} uses inline compilation to generate compiled code 
  for optimization.

  \code{ccoptim} provides a compiled code interface ot R's function optim.
}
\usage{
 compile.optim (func, jacfunc = NULL,  
      declaration = character(), includes = character(), language = "F95", ...)

 ccoptim(par, fn, gr = NULL, ..., method = c("Nelder-Mead",        
     "BFGS", "CG", "L-BFGS-B", "SANN"), lower = -Inf, upper = Inf,   
     control = list(), hessian = FALSE, dllname = NULL, rpar = NULL, 
     ipar = NULL) 
}
\arguments{
  \item{par }{Initial values for the parameters to be optimized over.
  }
  \item{fn }{A compiled subroutine that defines the function to be minimized 
    (or maximized), e.g. as generated by \code{compile.optim} 
    The subroutine should be defined as \code{fn(n, x, f, rpar, ipar)},
     where \code{x} is the vector of parameters over which minimization is 
     to take place, \code{n} is the number of parameters, \code{rpar} and 
     \code{ipar} are vectors of double and integer values, as passed with the 
     arguments of the same name. The result, a scalar should be put in \code{f}.
  }
  \item{gr }{A compiled subroutine that specifies the gradient for the 
    "BFGS", "CG" and "L-BFGS-B" methods. 
    If it is NULL, a finite-difference approximation will be used.
    KARLINE: NEED TO TEST THIS or remove SANN
    For the "SANN" method it specifies a function to generate a new candidate point. 
    If it is NULL a default Gaussian Markov kernel is used.
    The subroutine should be defined as \code{gr(n, x, df, rpar, ipar)},
    where \code{x} is the vector of parameters over which minimization is 
     to take place, \code{n} is the number of parameters, \code{rpar} and 
     \code{ipar} are vectors of double and integer values, as passed with the
     arguments of the same name. The result, a vector of length \code{n} 
     should be put in \code{df}.
   }
   \item{method }{The method to be used. See ‘Details’ of \link{optim}.
   }
   \item{lower, upper }{Bounds on the variables for the "L-BFGS-B" method, 
     or bounds in which to search for method "Brent".
   }
   \item{control }{A list of control parameters. See ‘Details’ of \link{optim}.
   }
   \item{hessian }{Logical. Should a numerically differentiated Hessian matrix 
     be returned?
   }  
   \item{rpar, ipar }{double and integer vector to be passed upon running the model.
   } 
  \item{func }{A character vector with F95, Fortran, C or C++ code, without 
    declarations, that specifies the optimization function. 
    The user gets as input a vector \code{x} with the x-values, 
    and must specify the function value whose optimum is sought, one value, \code{f}.
  }
  \item{jacfunc }{A character vector with F95, Fortran, C or C++ code, without 
    declarations, that specifies the jacobian of the function whose optimim 
    has to be found. The user gets as input a vector \code{x} with the x-values, 
    and must specify the jacobian as a vector in \code{df}.
  }
  \item{declaration }{Text that eneters the declaration section in each function.
  }
  \item{includes }{Code that comes before the functions.
  }
  \item{language }{A character vector that specifies the source code; one of c("F95", "Fortran", "C++", "C") defaults to "F95".
  }
  \item{...}{optional arguments to the generic function (not used).
  }
}
\value{
  \code{compile.optim} returns an object of class \code{CFunc} or 
  \code{CFuncList}, as from the package  \code{inline}. 
  
  Each object of class \code{CFunc} can be called with the appropriate arguments
}  

\details{

The compiled function that is generated by \code{compile.optim} is defined as:

 \code{func(n, x, f, rpar, ipar) }

 \code{jacfunc(n, x, df, rpar, ipar) }
 
In case Fortran or F95 is used, \code{n} is an integer value,

\code{ipar} and \code{rpar} are an integer vector, and double vector, that can be 
used to pass values at runtime.  

\code{x}, and \code{df} are a double vector of length \code{n}.

\code{f} is one value, the function to optimize.

In case C or C++ are used all are pointers. 

the values of \code{df} are set to 0 at the beginning of the subroutine.

The user needs to specify \code{f, df}, based on \code{x}

}

\note{


}
\examples{
\dontrun{

## =======================================================================
## example 1  
## small Rosenbrock Banana function (as from example page of optim)
## =======================================================================

fr <- function(x) {   ## Rosenbrock Banana function
    x1 <- x[1]
    x2 <- x[2]
    100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr <- function(x) { ## Gradient of 'fr'
    x1 <- x[1]
    x2 <- x[2]
    c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
       200 *      (x2 - x1 * x1))
}

## ---------------------------------------------------------------------------
## The fortran version 
## ---------------------------------------------------------------------------

fr.f95 <- "
  f = 100.d0 * (x(2) - x(1) * x(1))**2 + (1. - x(1))**2"

grr.f95 <- "
  df(1) = -400.d0 * x(1) *(x(2) - x(1) * x(1)) -2.d0* (1. - x(1))
  df(2) = 200d0          *(x(2) - x(1) * x(1))"

ccfr  <- compile.optim(func = fr.f95, jacfunc = grr.f95)

optim(c(-1.2,1), fr)
ccoptim(c(-1.2, 1), ccfr)
ccoptim(c(-1.2,1), ccfr, method = "BFGS")
## These do not converge in the default number of steps
optim(c(-1.2,1), fr, grr, method = "CG")
ccoptim(c(-1.2,1), ccfr, method = "CG")
optim(c(-1.2,1), fr, grr, method = "CG", control = list(type = 2))
ccoptim(c(-1.2,1), ccfr, method = "CG", control = list(type = 2))
optim(c(-1.2,1), fr, grr, method = "L-BFGS-B")
ccoptim(c(-1.2,1), ccfr, method = "L-BFGS-B")

## =======================================================================
## example 2 -  a larger Rosenbrock function  (as from optimx package)
## =======================================================================

genrose.f <- function(x, gs=NULL){ # objective function
	n <- length(x)
  ii <- seq(1, n-1, by = 2)
 	fval <- sum (gs*(x[ii+1]^2 - x[ii])^2 + (x[ii] - 1)^2)
        return(fval)
}

startx <- 4*seq(1:1000)/3.

print(system.time(ans <- optim(startx, fn = genrose.f, gs=100)))

genrose.f95 <- "
  integer i
  f = 0.d0
  do i = 1, n-1, 2
    f = f + rpar(1) * (x(i+1)**2 - x(i))**2 + (x(i) - 1)**2
  enddo
  f = 1.d0 + f
"
cGenrose <- compile.optim(genrose.f95)
print(system.time(anscc <- ccoptim(startx, fn = cGenrose, rpar = 100)))


}
}
\author{
  Karline Soetaert <karline.soetaert@nioz.nl>
}

\keyword{ utilities }